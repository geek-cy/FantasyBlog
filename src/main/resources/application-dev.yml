spring:
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://127.0.0.1:3306/blog?useUnicode=true&characterEncoding=utf8&autoReconnect=true&useSSL=false&serverTimezone=UTC
    username: root
    password: admin
    # 连接池指定 springboot2.02版本默认使用HikariCP 此处要替换成Druid
    # 下面为连接池的补充设置，应用到上面所有数据源中
    type: com.alibaba.druid.pool.DruidDataSource
    druid:
      # 初始化大小，最小，最大
      initial-size: 1
      min-idle: 3
      max-active: 20
      max-wait: 60000
      # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
      time-between-eviction-runs-millis: 30000
      # 配置一个连接在池中最小生存的时间，单位是毫秒
      min-evictable-idle-time-millis: 30000
      validation-query: select 'x'
      test-while-idle: true
      test-on-borrow: false
      test-on-return: false
      max-pool-prepared-statement-per-connection-size: 20
      # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙会拦截config
      filters: stat,slf4j,wall
#      filter:
#        wall:
#          config:
#            multi-statement-allow: true
      # 通过connectProperties属性来打开mergeSql功能；慢SQL记录
      connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000

  redis:
    database: 0  #数据库索引
    host: 121.41.164.231
    port: 6380
    password:
    timeout: 5000   #连接超时时间

  thymeleaf:
    cache: false

  servlet:
    multipart:
      max-file-size: 200MB # 设置传输文件大小
      max-request-size: 200MB

#  kafka:
#    bootstrap-servers: 127.0.0.1:9092
#    consumer:
#      group-id: blog-consumer-group # 群组ID
#      enable-auto-commit: true # 开启自动提交
#      auto-commit-interval: 3000 # 自动提交频率3s
#      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
#      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
#    producer:
#      key-serializer: org.apache.kafka.common.serialization.StringSerializer
#      value-serializer: org.apache.kafka.common.serialization.StringSerializer
#    listener:
#      missing-topics-fatal: false

  mail:
    host: smtp.qq.com #发送邮件服务器
    username: geek_cy@qq.com #QQ邮箱
    password: tdyoeatcudjdbchf #客户端授权码
    protocol: smtp #发送邮件协议
    properties.mail.smtp.auth: true
    properties.mail.smtp.port: 465
    properties.mail.display.sendmail: FantasyBlog #可以任意
    properties.mail.display.sendname: Spring Boot FantasyBlog Email #可以任意
    properties.mail.smtp.starttls.enable: true
    properties.mail.smtp.starttls.required: true
    properties.mail.smtp.ssl.enable: true
    default-encoding: utf-8
    from: geek_cy@qq.com #与上面的username保持一致

  jackson:
    date-format: yyyy-MM-dd HH:mm:ss
    # 时区设置
    time-zone: GMT+8

  # Spring线程池
  task:
    execution:
      pool:
        core-size: 5
        max-size: 15
        queue-capacity: 100
    scheduling:
      pool:
        size: 5
  quartz:
    job-store-type: jdbc #默认读取内存,这里使其存入数据库
    scheduler-name: blogSchedule
    properties:
      org:
        quartz:
          scheduler:
            instanceId: AUTO #调度器ID自动生成
          jobStore:
            class: org.quartz.impl.jdbcjobstore.JobStoreTX # 存数据库使用到的类
            driverDelegateClass: org.quartz.impl.jdbcjobstore.StdJDBCDelegate # 驱动
            isClustered: true #启用集群
          threadPool:
            class: org.quartz.simpl.SimpleThreadPool #使用的线程池
            threadCount: 5

mybatis-plus:
  configuration:
#    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl # 输出日志
    map-underscore-to-camel-case: true  # 开启驼峰映射
    aggressive-lazy-loading: false # 懒加载
  type-aliases-package: cn.fantasyblog.entity  # 直接使用类名

server:
  address: 127.0.0.1
  port: 8000

elasticsearch:
  host: 127.0.0.1
  port: 9200

file:
  path: F:\fantasyblog\
  avatar: F:\fantasyblog\avatar\
  maxSize: 200

logging:
  level:
    cn.fantasyblog: DEBUG

